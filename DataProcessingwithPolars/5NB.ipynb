{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8be0da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution time: 0.0823s\n",
      "Lazy execution time: 0.0125s\n",
      "Speedup: 6.59x\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "\n",
    "# Sample large dataset\n",
    "large_df = pl.DataFrame({\n",
    "    'id': range(1000000),\n",
    "    'category': ['A', 'B', 'C', 'D'] * 250000,\n",
    "    'value': range(1000000),\n",
    "    'amount': [i * 0.1 for i in range(1000000)]\n",
    "})\n",
    "\n",
    "# Eager evaluation (immediate execution)\n",
    "start_time = time.time()\n",
    "eager_result = (\n",
    "    large_df\n",
    "    .filter(pl.col('category') == 'A')\n",
    "    .group_by('category')\n",
    "    .agg(pl.col('amount').sum())\n",
    "    .sort('amount')\n",
    ")\n",
    "eager_time = time.time() - start_time\n",
    "\n",
    "# Lazy evaluation (deferred execution)\n",
    "start_time = time.time()\n",
    "lazy_result = (\n",
    "    large_df.lazy()\n",
    "    .filter(pl.col('category') == 'A')\n",
    "    .group_by('category')\n",
    "    .agg(pl.col('amount').sum())\n",
    "    .sort('amount')\n",
    "    .collect()  # Only executes here\n",
    ")\n",
    "lazy_time = time.time() - start_time\n",
    "\n",
    "print(f\"Eager execution time: {eager_time:.4f}s\")\n",
    "print(f\"Lazy execution time: {lazy_time:.4f}s\")\n",
    "print(f\"Speedup: {eager_time/lazy_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fac9d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The system cannot find the file specified. (os error 2): large_file.csv\n\nThis error occurred with the following context stack:\n\t[1] 'csv scan'\n\t[2] 'select'\n\t[3] 'filter'\n\t[4] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Projection pushdown: Select only needed columns early\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This reduces memory usage and I/O\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Efficient: Use lazy API with projection pushdown\u001b[39;00m\n\u001b[32m      8\u001b[39m efficient_result = (\n\u001b[32m      9\u001b[39m     \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscan_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlarge_file.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcol1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcol2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcol3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only these columns are read\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcol1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Example with in-memory DataFrame\u001b[39;00m\n\u001b[32m     16\u001b[39m result = (\n\u001b[32m     17\u001b[39m     large_df.lazy()\n\u001b[32m     18\u001b[39m     .select([\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mamount\u001b[39m\u001b[33m'\u001b[39m])  \u001b[38;5;66;03m# Project early\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     .collect()\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heirm\\Documents\\DE Learning\\tutorial-env\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heirm\\Documents\\DE Learning\\tutorial-env\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heirm\\Documents\\DE Learning\\tutorial-env\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2429\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2427\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2428\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: The system cannot find the file specified. (os error 2): large_file.csv\n\nThis error occurred with the following context stack:\n\t[1] 'csv scan'\n\t[2] 'select'\n\t[3] 'filter'\n\t[4] 'sink'\n"
     ]
    }
   ],
   "source": [
    "# Projection pushdown: Select only needed columns early\n",
    "# This reduces memory usage and I/O\n",
    "\n",
    "# Inefficient: Read all columns, then select\n",
    "# result = pl.read_csv('large_file.csv').select(['col1', 'col2', 'col3'])\n",
    "\n",
    "# Efficient: Use lazy API with projection pushdown\n",
    "efficient_result = (\n",
    "    pl.scan_csv('large_file.csv')\n",
    "    .select(['col1', 'col2', 'col3'])  # Only these columns are read\n",
    "    .filter(pl.col('col1') > 100)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Example with in-memory DataFrame\n",
    "result = (\n",
    "    large_df.lazy()\n",
    "    .select(['id', 'category', 'amount'])  # Project early\n",
    "    .filter(pl.col('amount') > 500)\n",
    "    .group_by('category')\n",
    "    .agg(pl.col('amount').mean())\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"Projection pushdown applied automatically in lazy evaluation\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c143fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized query plan:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "The system cannot find the file specified. (os error 2): sales_data.csv\n\nThis error occurred with the following context stack:\n\t[1] 'csv scan'\n\t[2] 'join left'\n\t[3] 'join'\n\t[4] 'filter'\n\t[5] 'filter'\n\t[6] 'group_by'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# View the optimized query plan\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptimized query plan:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlazy_query\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Execute the optimized query\u001b[39;00m\n\u001b[32m     25\u001b[39m result = lazy_query.collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heirm\\Documents\\DE Learning\\tutorial-env\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heirm\\Documents\\DE Learning\\tutorial-env\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heirm\\Documents\\DE Learning\\tutorial-env\\Lib\\site-packages\\polars\\lazyframe\\frame.py:1391\u001b[39m, in \u001b[36mLazyFrame.explain\u001b[39m\u001b[34m(self, format, optimized, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, streaming, engine, tree_format, optimizations)\u001b[39m\n\u001b[32m   1389\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ldf.describe_optimized_plan_tree()\n\u001b[32m   1390\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdescribe_optimized_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mtree\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ldf.describe_plan_tree()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: The system cannot find the file specified. (os error 2): sales_data.csv\n\nThis error occurred with the following context stack:\n\t[1] 'csv scan'\n\t[2] 'join left'\n\t[3] 'join'\n\t[4] 'filter'\n\t[5] 'filter'\n\t[6] 'group_by'\n"
     ]
    }
   ],
   "source": [
    "# Predicate pushdown: Apply filters as early as possible\n",
    "# This reduces the amount of data processed in subsequent operations\n",
    "\n",
    "# Polars automatically pushes predicates down in lazy evaluation\n",
    "lazy_query = (\n",
    "    pl.scan_csv('sales_data.csv')\n",
    "    .join(\n",
    "        pl.scan_csv('customers.csv'),\n",
    "        on='customer_id'\n",
    "    )\n",
    "    .filter(pl.col('purchase_date') >= '2024-01-01')  # Filter pushed down\n",
    "    .filter(pl.col('amount') > 100)  # Multiple filters combined\n",
    "    .group_by('customer_segment')\n",
    "    .agg([\n",
    "        pl.col('amount').sum().alias('total_sales'),\n",
    "        pl.col('amount').count().alias('transaction_count')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# View the optimized query plan\n",
    "print(\"Optimized query plan:\")\n",
    "print(lazy_query.explain())\n",
    "\n",
    "# Execute the optimized query\n",
    "result = lazy_query.collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use appropriate data types to reduce memory usage\n",
    "def optimize_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Optimize data types for memory efficiency.\"\"\"\n",
    "    return df.with_columns([\n",
    "        # Use smaller integer types when possible\n",
    "        pl.when(pl.col('id').max() < 32767)\n",
    "        .then(pl.col('id').cast(pl.Int16))\n",
    "        .when(pl.col('id').max() < 2147483647)\n",
    "        .then(pl.col('id').cast(pl.Int32))\n",
    "        .otherwise(pl.col('id'))\n",
    "        .alias('id'),\n",
    "        \n",
    "        # Use categorical for repeated strings\n",
    "        pl.col('category').cast(pl.Categorical),\n",
    "        \n",
    "        # Use Float32 instead of Float64 when precision allows\n",
    "        pl.col('amount').cast(pl.Float32)\n",
    "    ])\n",
    "\n",
    "# Apply optimizations\n",
    "optimized_df = optimize_dtypes(large_df)\n",
    "\n",
    "# Compare memory usage\n",
    "original_memory = large_df.estimated_size('mb')\n",
    "optimized_memory = optimized_df.estimated_size('mb')\n",
    "\n",
    "print(f\"Original memory usage: {original_memory:.2f} MB\")\n",
    "print(f\"Optimized memory usage: {optimized_memory:.2f} MB\")\n",
    "print(f\"Memory savings: {((original_memory - optimized_memory) / original_memory * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For datasets larger than memory, use streaming\n",
    "# Note: Streaming is available for certain operations\n",
    "\n",
    "# Example: Process a very large CSV file in chunks\n",
    "def process_large_file_streaming(file_path: str):\n",
    "    \"\"\"\n",
    "    Process a large file using streaming to handle data larger than memory.\n",
    "    \"\"\"\n",
    "    result = (\n",
    "        pl.scan_csv(file_path)\n",
    "        .filter(pl.col('amount') > 1000)\n",
    "        .group_by('category')\n",
    "        .agg([\n",
    "            pl.col('amount').sum().alias('total_amount'),\n",
    "            pl.col('amount').count().alias('count')\n",
    "        ])\n",
    "        .collect(streaming=True)  # Enable streaming\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Streaming aggregations work well for:\n",
    "# - Group by operations\n",
    "# - Simple filters and projections\n",
    "# - Window functions (in some cases)\n",
    "\n",
    "print(\"Streaming enabled for large dataset processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize joins for better performance\n",
    "\n",
    "# Create sample DataFrames for join examples\n",
    "left_df = pl.DataFrame({\n",
    "    'key': range(100000),\n",
    "    'left_value': range(100000)\n",
    "})\n",
    "\n",
    "right_df = pl.DataFrame({\n",
    "    'key': range(0, 100000, 2),  # Every other key\n",
    "    'right_value': range(50000)\n",
    "})\n",
    "\n",
    "# Efficient join with lazy evaluation\n",
    "efficient_join = (\n",
    "    left_df.lazy()\n",
    "    .join(\n",
    "        right_df.lazy(),\n",
    "        on='key',\n",
    "        how='inner'\n",
    "    )\n",
    "    .filter(pl.col('left_value') > 1000)  # Filter after join\n",
    "    .select(['key', 'left_value', 'right_value'])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# For very large joins, consider:\n",
    "# 1. Sorting both DataFrames on join key\n",
    "# 2. Using appropriate join strategy\n",
    "# 3. Filtering before joining when possible\n",
    "\n",
    "# Pre-filter before join for better performance\n",
    "optimized_join = (\n",
    "    left_df.lazy()\n",
    "    .filter(pl.col('left_value') > 1000)  # Filter before join\n",
    "    .join(\n",
    "        right_df.lazy(),\n",
    "        on='key',\n",
    "        how='inner'\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"Efficient join result shape: {efficient_join.shape}\")\n",
    "print(f\"Optimized join result shape: {optimized_join.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bcefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always prefer vectorized operations over Python loops\n",
    "\n",
    "# Inefficient: Python loop\n",
    "def slow_calculation(df):\n",
    "    results = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        if row['value'] > 500000:\n",
    "            results.append(row['value'] * 2)\n",
    "        else:\n",
    "            results.append(row['value'])\n",
    "    return results\n",
    "\n",
    "# Efficient: Vectorized operation\n",
    "def fast_calculation(df):\n",
    "    return df.with_columns(\n",
    "        pl.when(pl.col('value') > 500000)\n",
    "        .then(pl.col('value') * 2)\n",
    "        .otherwise(pl.col('value'))\n",
    "        .alias('calculated_value')\n",
    "    )\n",
    "\n",
    "# Time comparison\n",
    "start_time = time.time()\n",
    "slow_result = slow_calculation(large_df.head(10000))\n",
    "slow_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "fast_result = fast_calculation(large_df.head(10000))\n",
    "fast_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop-based calculation: {slow_time:.4f}s\")\n",
    "print(f\"Vectorized calculation: {fast_time:.4f}s\")\n",
    "print(f\"Speedup: {slow_time/fast_time:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and understand query execution plans\n",
    "\n",
    "complex_query = (\n",
    "    large_df.lazy()\n",
    "    .filter(pl.col('category').is_in(['A', 'B']))\n",
    "    .with_columns([\n",
    "        (pl.col('value') * pl.col('amount')).alias('total_value')\n",
    "    ])\n",
    "    .group_by('category')\n",
    "    .agg([\n",
    "        pl.col('total_value').sum().alias('sum_total_value'),\n",
    "        pl.col('total_value').mean().alias('avg_total_value'),\n",
    "        pl.count().alias('count')\n",
    "    ])\n",
    "    .filter(pl.col('count') > 1000)\n",
    "    .sort('sum_total_value', descending=True)\n",
    ")\n",
    "\n",
    "# View the optimized execution plan\n",
    "print(\"Query execution plan:\")\n",
    "print(complex_query.explain())\n",
    "\n",
    "# View the unoptimized plan (for comparison)\n",
    "print(\"\\nUnoptimized plan:\")\n",
    "print(complex_query.explain(optimized=False))\n",
    "\n",
    "# Execute the query\n",
    "result = complex_query.collect()\n",
    "print(f\"\\nQuery result shape: {result.shape}\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Polars for optimal parallel processing\n",
    "\n",
    "# Check current thread configuration\n",
    "print(f\"Available threads: {pl.thread_pool_size()}\")\n",
    "\n",
    "# Set thread pool size (usually auto-detected optimally)\n",
    "# pl.set_thread_pool_size(8)  # Set to specific number if needed\n",
    "\n",
    "# Polars automatically parallelizes:\n",
    "# - Column operations\n",
    "# - Group by operations  \n",
    "# - Joins\n",
    "# - Aggregations\n",
    "# - I/O operations\n",
    "\n",
    "# Example of parallel group by\n",
    "parallel_groupby = (\n",
    "    large_df.lazy()\n",
    "    .group_by('category')\n",
    "    .agg([\n",
    "        pl.col('value').sum(),\n",
    "        pl.col('value').mean(),\n",
    "        pl.col('value').std(),\n",
    "        pl.col('value').min(),\n",
    "        pl.col('value').max()\n",
    "    ])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"Parallel group by completed\")\n",
    "print(parallel_groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bab383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the right file format for performance\n",
    "\n",
    "# Parquet is generally fastest for analytical workloads\n",
    "start_time = time.time()\n",
    "parquet_df = pl.scan_parquet('data.parquet').collect()\n",
    "parquet_time = time.time() - start_time\n",
    "\n",
    "# CSV is slower but more universal\n",
    "start_time = time.time()\n",
    "csv_df = pl.scan_csv('data.csv').collect()\n",
    "csv_time = time.time() - start_time\n",
    "\n",
    "print(f\"Parquet read time: {parquet_time:.4f}s\")\n",
    "print(f\"CSV read time: {csv_time:.4f}s\")\n",
    "print(f\"Parquet speedup: {csv_time/parquet_time:.2f}x\")\n",
    "\n",
    "# For writing, also prefer Parquet\n",
    "large_df.write_parquet('output.parquet', compression='snappy')\n",
    "print(\"Data written to Parquet with Snappy compression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-env (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
